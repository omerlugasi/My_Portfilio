{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fa6f881-0320-4806-ad83-c0c88be54c36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Section 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "755749df-9d67-4cb4-b081-c134c49ead2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We chose to filter out rows with missing or invalid values for key fields.\n",
    "\n",
    "This decision is based on guidance provided in the forum and the homework instructions, which state that if a row lacks the necessary information to determine viewing behavior or wealth characteristics, it can be safely disregarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92a63ddd-4161-492d-b519-ef19b7b16934",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import sum as spark_sum, max as spark_max\n",
    "from pyspark.sql.types import *\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ae3dd684-9cb3-4432-885e-db1848b07839",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_csv_file(filename, schema):\n",
    "  # Reads the relevant file from distributed file system using the given schema\n",
    "\n",
    "  allowed_files = {'Daily program data': ('Daily program data', \"|\"),\n",
    "                   'demographic': ('demographic', \"|\")}\n",
    "\n",
    "  if filename not in allowed_files.keys():\n",
    "    print(f'You were trying to access unknown file \\\"{filename}\\\". Only valid options are {allowed_files.keys()}')\n",
    "    return None\n",
    "\n",
    "  filepath = allowed_files[filename][0]\n",
    "  dataPath = f\"dbfs:/mnt/coursedata2024/fwm-stb-data/{filepath}\"\n",
    "  delimiter = allowed_files[filename][1]\n",
    "\n",
    "  df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"false\")\\\n",
    "    .option(\"delimiter\",delimiter)\\\n",
    "    .schema(schema)\\\n",
    "    .load(dataPath)\n",
    "  return df\n",
    "schemas_dict = {'Daily program data':\n",
    "                  StructType([\n",
    "                    StructField('prog_code', StringType()),\n",
    "                    StructField('title', StringType()),\n",
    "                    StructField('genre', StringType()),\n",
    "                    StructField('air_date', StringType()),\n",
    "                    StructField('air_time', StringType()),\n",
    "                    StructField('Duration', FloatType())\n",
    "                  ]),\n",
    "                'viewing':\n",
    "                  StructType([\n",
    "                    StructField('device_id', StringType()),\n",
    "                    StructField('event_date', StringType()),\n",
    "                    StructField('event_time', IntegerType()),\n",
    "                    StructField('mso_code', StringType()),\n",
    "                    StructField('prog_code', StringType()),\n",
    "                    StructField('station_num', StringType())\n",
    "                  ]),\n",
    "                'viewing_full':\n",
    "                  StructType([\n",
    "                    StructField('mso_code', StringType()),\n",
    "                    StructField('device_id', StringType()),\n",
    "                    StructField('event_date', IntegerType()),\n",
    "                    StructField('event_time', IntegerType()),\n",
    "                    StructField('station_num', StringType()),\n",
    "                    StructField('prog_code', StringType())\n",
    "                  ]),\n",
    "                'demographic':\n",
    "                  StructType([StructField('household_id',StringType()),\n",
    "                    StructField('household_size',IntegerType()),\n",
    "                    StructField('num_adults',IntegerType()),\n",
    "                    StructField('num_generations',IntegerType()),\n",
    "                    StructField('adult_range',StringType()),\n",
    "                    StructField('marital_status',StringType()),\n",
    "                    StructField('race_code',StringType()),\n",
    "                    StructField('presence_children',StringType()),\n",
    "                    StructField('num_children',IntegerType()),\n",
    "                    StructField('age_children',StringType()), #format like range - 'bitwise'\n",
    "                    StructField('age_range_children',StringType()),\n",
    "                    StructField('dwelling_type',StringType()),\n",
    "                    StructField('home_owner_status',StringType()),\n",
    "                    StructField('length_residence',IntegerType()),\n",
    "                    StructField('home_market_value',StringType()),\n",
    "                    StructField('num_vehicles',IntegerType()),\n",
    "                    StructField('vehicle_make',StringType()),\n",
    "                    StructField('vehicle_model',StringType()),\n",
    "                    StructField('vehicle_year',IntegerType()),\n",
    "                    StructField('net_worth',IntegerType()),\n",
    "                    StructField('income',StringType()),\n",
    "                    StructField('gender_individual',StringType()),\n",
    "                    StructField('age_individual',IntegerType()),\n",
    "                    StructField('education_highest',StringType()),\n",
    "                    StructField('occupation_highest',StringType()),\n",
    "                    StructField('education_1',StringType()),\n",
    "                    StructField('occupation_1',StringType()),\n",
    "                    StructField('age_2',IntegerType()),\n",
    "                    StructField('education_2',StringType()),\n",
    "                    StructField('occupation_2',StringType()),\n",
    "                    StructField('age_3',IntegerType()),\n",
    "                    StructField('education_3',StringType()),\n",
    "                    StructField('occupation_3',StringType()),\n",
    "                    StructField('age_4',IntegerType()),\n",
    "                    StructField('education_4',StringType()),\n",
    "                    StructField('occupation_4',StringType()),\n",
    "                    StructField('age_5',IntegerType()),\n",
    "                    StructField('education_5',StringType()),\n",
    "                    StructField('occupation_5',StringType()),\n",
    "                    StructField('polit_party_regist',StringType()),\n",
    "                    StructField('polit_party_input',StringType()),\n",
    "                    StructField('household_clusters',StringType()),\n",
    "                    StructField('insurance_groups',StringType()),\n",
    "                    StructField('financial_groups',StringType()),\n",
    "                    StructField('green_living',StringType())\n",
    "                  ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "feb08d7e-cd54-432e-a31a-7b34c7e57222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load datasets using pre-defined schema and helper\n",
    "program_df = load_csv_file('Daily program data', schemas_dict['Daily program data'])\n",
    "demographic_df = load_csv_file('demographic', schemas_dict['demographic'])\n",
    "\n",
    "# Load reference data (parquet)\n",
    "ref_data_schema = StructType([\n",
    "    StructField('device_id', StringType()),\n",
    "    StructField('dma', StringType()),\n",
    "    StructField('dma_code', StringType()),\n",
    "    StructField('household_id', IntegerType()),\n",
    "    StructField('zipcode', IntegerType())\n",
    "])\n",
    "reference_df = spark.read.format('parquet') \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"dbfs:/FileStore/ddm/ref_data\")\n",
    "\n",
    "# Load viewing data (CSV with schema)\n",
    "viewing_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .schema(schemas_dict['viewing_full']) \\\n",
    "    .load(\"dbfs:/FileStore/ddm/10m_viewing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03f65c4f-477a-42e2-9a44-3c7765004cca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "viewing_df.dropDuplicates()\n",
    "program_df.dropDuplicates()\n",
    "demographic_df.dropDuplicates()\n",
    "reference_df.dropDuplicates()\n",
    "\n",
    "\n",
    "viewing_df = viewing_df.filter(col(\"prog_code\").isNotNull() &\n",
    "                               col(\"device_id\").isNotNull())\n",
    "\n",
    "program_df = program_df.filter(col(\"title\").isNotNull() &\n",
    "                               col(\"genre\").isNotNull() &\n",
    "                               col(\"prog_code\").isNotNull()).dropDuplicates([\"prog_code\", \"genre\"])\n",
    "\n",
    "demographic_df = demographic_df.filter(col(\"household_id\").isNotNull()).dropDuplicates([\"household_id\"])\n",
    "\n",
    "\n",
    "reference_df = reference_df.filter(col(\"household_id\").isNotNull() &\n",
    "                                   col(\"DMA\").isNotNull() &\n",
    "                                   (col(\"DMA\") != \"Unknown\"))\n",
    "\n",
    "reference_df = reference_df.withColumn(\"household_id\",\n",
    "    lpad(col(\"household_id\").cast(\"string\"), 8, \"0\")  # Pad with zeros to 8 characters\n",
    ")\n",
    "\n",
    "demographic_df = demographic_df.withColumn(\"num_adults\", col(\"num_adults\").cast(\"int\")) \\\n",
    "    .withColumn(\"income\", \n",
    "                when(col(\"income\").rlike(\"^[A-D]$\"), \n",
    "                     ascii(substring(col(\"income\"), 1, 1)) - ascii(lit(\"A\")) + 10)\n",
    "                .otherwise(col(\"income\").cast(\"int\"))\n",
    "    ).withColumn(\"household_id\",\n",
    "                lpad(col(\"household_id\").cast(\"string\"), 8, \"0\")  # Pad with zeros to 8 characters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f654a16-7722-4b7b-bf9a-12d35d82037b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Genres:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "436d8ac4-6a8a-44a2-b169-79e7c43e8ab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n|  genre|total_viewers|\n+-------+-------------+\n|   News|       615298|\n|Reality|       610470|\n|   Talk|       537717|\n| Comedy|       509668|\n| Sitcom|       502748|\n+-------+-------------+\n\nTotal viewers for top 5 genres: 2775901\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Join viewing_df with cleaned program_df to get genre\n",
    "view_prog_df = viewing_df.join(program_df.select(\"prog_code\", \"genre\"), on=\"prog_code\")\n",
    "\n",
    "# STEP 2: Join with reference_df to get household_id\n",
    "view_prog_ref_df = view_prog_df.join(reference_df.select(\"device_id\", \"household_id\"), on=\"device_id\")\n",
    "\n",
    "# STEP 3: Join with demographic_df (broadcasted for performance)\n",
    "view_prog_ref_demo_df = view_prog_ref_df.join(\n",
    "    broadcast(demographic_df.select(\"household_id\", \"household_size\")),\n",
    "    on=\"household_id\"\n",
    ")\n",
    "\n",
    "# STEP 4: Explode comma-separated genres into separate rows\n",
    "exploded_genres_df = view_prog_ref_demo_df.withColumn(\"genre\", explode(split(col(\"genre\"), \",\\\\s*\")))\n",
    "\n",
    "# STEP 5: Drop duplicates (one row per household per genre)\n",
    "unique_viewers = exploded_genres_df \\\n",
    "    .filter(col(\"genre\").isNotNull() & (col(\"genre\") != \"\")) \\\n",
    "    .select(\"household_id\", \"genre\", \"household_size\") \\\n",
    "    .dropDuplicates()\n",
    "\n",
    "# STEP 6: Aggregate total household size per genre\n",
    "genre_agg_df = (\n",
    "    unique_viewers\n",
    "    .groupBy(\"genre\")\n",
    "    .agg(spark_sum(\"household_size\").alias(\"total_viewers\"))\n",
    "    .orderBy(col(\"total_viewers\").desc())\n",
    ")\n",
    "# STEP 7: Show top 5 genres\n",
    "top5_genres = genre_agg_df.limit(5)\n",
    "top5_genres.cache()\n",
    "top5_genres.show()\n",
    "\n",
    "# STEP 8: Show total viewers across top 5\n",
    "total_viewers_top5 = top5_genres.agg(spark_sum(\"total_viewers\")).collect()[0][0]\n",
    "print(\"Total viewers for top 5 genres:\", total_viewers_top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b08f8099-73c1-40b4-8b6e-39a9aa7bc9c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**DMAs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f680a319-f179-42f5-90dc-e02897c38586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+------------+\n|DMA                       |total_people|\n+--------------------------+------------+\n|Little Rock-Pine Bluff    |31652       |\n|Seattle-Tacoma            |35124       |\n|Toledo                    |24108       |\n|Wilkes Barre-Scranton-Hztn|42844       |\n|Charleston-Huntington     |60656       |\n+--------------------------+------------+\n\nTotal people in top 5 DMAs: 194384\n"
     ]
    }
   ],
   "source": [
    "# Count devices per DMA using only reference data\n",
    "dma_device_counts = (\n",
    "    reference_df.groupBy(\"DMA\")\n",
    "    .agg(count(\"device_id\").alias(\"device_count\"))\n",
    "    .orderBy(col(\"device_count\").desc())\n",
    ")\n",
    "\n",
    "# Get top 5 DMAs\n",
    "top5_dmas = dma_device_counts.limit(5)\n",
    "\n",
    "# Next, Get total people in top 5 DMAs\n",
    "top5_dma_names = [row[\"DMA\"] for row in top5_dmas.collect()]\n",
    "\n",
    "dma_people_df = reference_df.join(demographic_df.select(\"household_id\", \"household_size\"), on=\"household_id\")\n",
    "dma_people_filtered = dma_people_df.filter(col(\"DMA\").isin(top5_dma_names))\n",
    "\n",
    "dma_totals = (\n",
    "    dma_people_filtered\n",
    "    .select(\"DMA\", \"household_id\", \"household_size\")\n",
    "    .dropDuplicates()\n",
    "    .groupBy(\"DMA\")\n",
    "    .agg(spark_sum(\"household_size\").alias(\"total_people\"))\n",
    ")\n",
    "\n",
    "dma_totals.cache()\n",
    "dma_totals.show(truncate=False)\n",
    "\n",
    "total_people_top5_dmas = dma_totals.agg(spark_sum(\"total_people\")).collect()[0][0]\n",
    "print(\"Total people in top 5 DMAs:\", total_people_top5_dmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cfdf25c-96f8-41ac-9f30-a2608ea68042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Program Titles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9059588-5cf8-43e3-acde-4f264e394659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+\n|              title|total_viewers|\n+-------------------+-------------+\n| College Basketball|        74693|\n|   Paid Programming|        73049|\n|       SportsCenter|        58833|\n|The Big Bang Theory|        53317|\n|              Today|        44178|\n+-------------------+-------------+\n\nTotal viewers in top 5 kids programs: 304070\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Filter early and reduce joins\n",
    "demographic_kids_df = demographic_df.filter(upper(col(\"presence_children\")) == \"Y\").select(\"household_id\", \"household_size\")\n",
    "\n",
    "viewing_with_household_df = viewing_df.join(broadcast(reference_df.select(\"device_id\", \"household_id\")), on=\"device_id\")\n",
    "viewing_kids_df = viewing_with_household_df.join(broadcast(demographic_kids_df), on=\"household_id\")\n",
    "\n",
    "viewing_kids_prog_df = viewing_kids_df.join(broadcast(program_df.select(\"prog_code\", \"title\")), on=\"prog_code\")\n",
    "\n",
    "# Step 2: Drop partial duplicates (not full row)\n",
    "unique_views = viewing_kids_prog_df.select(\"household_id\", \"title\", \"household_size\").dropDuplicates([\"household_id\", \"title\"])\n",
    "\n",
    "# Step 3: Repartition to avoid skew, then aggregate\n",
    "unique_views = unique_views.repartition(\"title\")\n",
    "\n",
    "top_kids_programs = (\n",
    "    unique_views\n",
    "    .groupBy(\"title\")\n",
    "    .agg(spark_sum(\"household_size\").alias(\"total_viewers\"))\n",
    "    .orderBy(col(\"total_viewers\").desc())\n",
    "    .limit(5)\n",
    ")\n",
    "\n",
    "top_kids_programs.cache()\n",
    "top_kids_programs.show()\n",
    "\n",
    "# Step 4: Sum\n",
    "total_viewers_top5_kids = top_kids_programs.agg(spark_sum(\"total_viewers\")).collect()[0][0]\n",
    "print(\"Total viewers in top 5 kids programs:\", total_viewers_top5_kids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d56f1ae3-6ca3-4ec9-be66-c64ec542984b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Section 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03bd18d5-e31f-40d5-8418-7d7dadef2fa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wealth Score per DMA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fba740f7-761d-4ebf-910e-6838ea3c9e72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "demographic_df = demographic_df.filter(col(\"net_worth\").isNotNull() &\n",
    "                                       col(\"income\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba16c394-d9f6-4af7-af55-e1da185454e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join reference with demographic to get DMA, net worth, and income\n",
    "dma_wealth_df = reference_df.select(\"household_id\", \"DMA\").join(\n",
    "    demographic_df.select(\"household_id\", \"net_worth\", \"income\"),\n",
    "    on=\"household_id\"\n",
    ")\n",
    "\n",
    "# Compute average net worth and income per DMA\n",
    "dma_avg_df = dma_wealth_df.groupBy(\"DMA\").agg(\n",
    "    avg(\"net_worth\").alias(\"avg_net_worth\"),\n",
    "    avg(\"income\").alias(\"avg_income\")\n",
    ")\n",
    "\n",
    "max_income = int(dma_wealth_df.agg(spark_max(\"income\")).collect()[0][0])\n",
    "max_net_worth = int(dma_wealth_df.agg(spark_max(\"net_worth\")).collect()[0][0])\n",
    "\n",
    "# Compute wealth score and sort\n",
    "dma_score_df = dma_avg_df.withColumn(\n",
    "    \"wealth_score\",\n",
    "    (col(\"avg_net_worth\") / max_net_worth) + (col(\"avg_income\") / max_income)\n",
    ").orderBy(col(\"wealth_score\").desc())\n",
    "\n",
    "# Select top 10 DMAs by wealth score\n",
    "top10_dmas = dma_score_df.limit(10).select(\"DMA\", \"wealth_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c3fa261-a608-4e97-ad06-a094c82d5a49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Genre Popularity per DMA, Then Rank Genres by Popularity Within Each DMA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3786a00d-e5d8-44b7-8222-cd67153bb29b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replace the join in Step 1 with this:\n",
    "view_prog_df = viewing_df.join(program_df.select(\"prog_code\", \"genre\"), on=\"prog_code\", how=\"inner\")\n",
    "\n",
    "view_prog_ref_df = view_prog_df.join(\n",
    "    reference_df.select(\"device_id\", \"DMA\", \"household_id\"),\n",
    "    on=\"device_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Step 2: Explode genre list into individual genres\n",
    "genre_popularity_df = (\n",
    "    view_prog_ref_df\n",
    "    .withColumn(\"single_genre\", explode(split(col(\"genre\"), \",\\\\s*\")))\n",
    "    .select(\"DMA\", \"device_id\", \"single_genre\")\n",
    "    .dropDuplicates([\"device_id\", \"single_genre\"])  # avoid double-counting devices watching the same genre\n",
    "    .groupBy(\"DMA\", \"single_genre\")\n",
    "    .agg(countDistinct(\"device_id\").alias(\"device_count\"))\n",
    ")\n",
    "# Step 3: Join with top 10 DMAs\n",
    "top_dma_genres = top10_dmas.join(genre_popularity_df, on=\"DMA\", how=\"left\")\n",
    "\n",
    "# Step 4: Rank genres by total household size within each DMA\n",
    "window_spec = Window.partitionBy(\"DMA\").orderBy(col(\"device_count\").desc())\n",
    "\n",
    "ranked_genres = top_dma_genres.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "# Sort genres by rank within each DMA, and collect (rank, genre) structs\n",
    "ranked_lists = (\n",
    "    ranked_genres\n",
    "    .select(\"DMA\", \"wealth_score\", \"rank\", \"single_genre\")\n",
    "    .orderBy(\"DMA\", \"rank\")\n",
    "    .groupBy(\"DMA\", \"wealth_score\")\n",
    "    .agg(collect_list(struct(\"rank\", \"single_genre\")).alias(\"ranked_genre_structs\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b87a38b4-84f8-4963-aacf-8a0b97c718a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Assign 11 Unique Genres per DMA (No Repeats Across DMAs)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e40906a-ef12-4c6b-b904-27bc7470c072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+------------------+-----------------------------------------------------------------------------------------------------------------------------+\n|DMA                       |Wealth_Score      |Ordered_List_Of_Genres                                                                                                       |\n+--------------------------+------------------+-----------------------------------------------------------------------------------------------------------------------------+\n|San Antonio               |1.623931623931624 |[]                                                                                                                           |\n|San Francisco-Oak-San Jose|1.5422277562565332|[Reality, News, Comedy, Music, Sitcom, Talk, Drama, Documentary, Adventure, Children, Action]                                |\n|Baltimore                 |1.5220570915273188|[]                                                                                                                           |\n|Sacramnto-Stkton-Modesto  |1.478945452542812 |[Entertainment, Crime drama, Consumer, Animated, Newsmagazine, Suspense, Fantasy, Crime, Special, Mystery, Sports event]     |\n|Bend, OR                  |1.4565329736126789|[Shopping, Sports non-event, Game show, House/garden, Educational, Law, Travel, Public affairs, Interview, Cooking, How-to]  |\n|Houston                   |1.435499606907069 |[Science fiction, Romance, Home improvement, Politics, History, Basketball, Religious, Sports talk, Paranormal, Horror, Soap]|\n|Austin                    |1.4256070844518498|[Bus./financial, Medical, Science, Animals, Western, Outdoors, Weather, Nature, Romance-comedy, Comedy-drama, Golf]          |\n|Miami-Ft. Lauderdale      |1.421015585721468 |[]                                                                                                                           |\n|Seattle-Tacoma            |1.4161407504201444|[Health, Historical drama, Auto, Fashion, Awards, War, Auto racing, Docudrama, Biography, Fishing, Community]                |\n|Detroit                   |1.3571052908383296|[Hockey, Variety, Football, Musical, Collectibles, Technology, Hunting, Baseball, Parenting, Auction, Anthology]             |\n+--------------------------+------------------+-----------------------------------------------------------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "top_dma_genres_list = sorted(ranked_lists.collect(), key=lambda x: -x[\"wealth_score\"])  # Sort by wealth descending\n",
    "\n",
    "used_genres = set()\n",
    "final_output = []\n",
    "\n",
    "for row in top_dma_genres_list:\n",
    "    dma = row[\"DMA\"]\n",
    "    score = row[\"wealth_score\"]\n",
    "    genre_structs = row[\"ranked_genre_structs\"]\n",
    "\n",
    "    # Sort by rank just in case\n",
    "    genre_structs = sorted(genre_structs, key=lambda x: x[\"rank\"])\n",
    "\n",
    "    selected = []\n",
    "    for g in genre_structs:\n",
    "        genre = g[\"single_genre\"]\n",
    "        if genre and genre not in used_genres:\n",
    "            selected.append(genre)\n",
    "            used_genres.add(genre)\n",
    "        if len(selected) == 11:\n",
    "            break\n",
    "\n",
    "    final_output.append((dma, score, selected))\n",
    "\n",
    "# Convert to DataFrame\n",
    "final_schema = [\"DMA\", \"Wealth_Score\", \"Ordered_List_Of_Genres\"]\n",
    "final_result_df = spark.createDataFrame(final_output, final_schema)\n",
    "\n",
    "final_result_df.orderBy(col(\"Wealth_Score\").desc()).show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "project1_part2_212536924_322836180",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}